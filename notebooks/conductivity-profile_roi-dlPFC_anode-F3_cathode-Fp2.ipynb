{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the effect of conductivity profile (dlPFC, unipolar)\n",
    "\n",
    "In this notebook, we evaluate the effect of the conductivity profile on the F3-Fp2 unipolar electrodes montage targetting the dorsolateral prefrontal cortex (dlPFC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import arviz as az\n",
    "import bambi as bmb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add utility package to path\n",
    "sys.path.append(os.environ.get(\"BRAINWEB_TDCS_CODE_DIR\", \"../code\"))\n",
    "from brainweb_tdcs import ROIS, EXPERIMENTS\n",
    "from brainweb_tdcs.study import get_experiments_for_roi\n",
    "from brainweb_tdcs.plot import (\n",
    "    display_side_by_side,\n",
    "    plot_conductivity_categorical,\n",
    "    plot_posterior,\n",
    ")\n",
    "\n",
    "# Set path data directory\n",
    "if \"BRAINWEB_TDCS_DATA_DIR\" not in os.environ:\n",
    "    os.environ[\"BRAINWEB_TDCS_DATA_DIR\"] = str((Path.cwd() / \"../data\").resolve())\n",
    "\n",
    "# Set the random seed\n",
    "RANDOM_SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "CAPTIONS = {\n",
    "    \"e\": \"Magnitude of the\\nelectric field\",\n",
    "    \"e_r\": \"Magnitude of the\\nnormal component of electric field\",\n",
    "}\n",
    "LABELS = {\n",
    "    \"e\": \"$| \\mathbf{e} |$ (mVm$^{-1}$)\",\n",
    "    \"e_r\": \"$| \\mathbf{e}_r |$ (mVm$^{-1}$)\",\n",
    "}\n",
    "FUNC_DICT = {\n",
    "    \"mean\": np.mean,\n",
    "    \"std\": np.std,\n",
    "    \"2.5%\": lambda x: np.percentile(x, 2.5),\n",
    "    \"97.5%\": lambda x: np.percentile(x, 97.5),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the experiment\n",
    "\n",
    "We first select the experiment we want to process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "experiment = EXPERIMENTS[3]\n",
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the data\n",
    "\n",
    "The data regarding the conductivity profile is already formatted. The `\"k\"` column contains the name of the conductivity profile (`\"reference\"` and `\"halton_i\"`) while the column `\"k_id\"` contains an integer ranging from $0$ to $20$ included ($0$ corresponding to the reference conductivity profile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = experiment.get_data()[[\"sub\", \"k\", \"k_id\", \"e\", \"e_r\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then display the results for the mangitude of the electric field $| \\pmb{e} |$ and of its component normal to the cortical surface $| \\pmb{e}_r |$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plot_conductivity_categorical(axs[0], \"e\", data, LABELS[\"e\"], CAPTIONS[\"e\"])\n",
    "plot_conductivity_categorical(axs[1], \"e_r\", data, LABELS[\"e_r\"], CAPTIONS[\"e_r\"])\n",
    "plt.show()\n",
    "columns = [\"mean\", \"std\", \"25%\", \"75%\"]\n",
    "display_side_by_side(\n",
    "    [\n",
    "        data.groupby(\"k\").describe()[\"e\"][columns],\n",
    "        data.groupby(\"k\").describe()[\"e_r\"][columns],\n",
    "    ],\n",
    "    [\"\", \"\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the base model\n",
    "\n",
    "We can now model the differences between the means of the populations obtained with the four anode displacements and the reference one using the following Bayesian model:\n",
    "$$\n",
    "| \\pmb{e} | \\sim \\mathcal{N}(\\mu_{| \\pmb{e} |}, \\sigma_{| \\pmb{e} |}^2), \\\\\n",
    "| \\pmb{e}_r | \\sim \\mathcal{N}(\\mu_{| \\pmb{e}_r |}, \\sigma_{| \\pmb{e}_r |}^2),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mu_{| \\pmb{e} |} = \\alpha_{| \\pmb{e} |} + \\sum_{k=1}^4 \\beta_{| \\pmb{e} |, k} \\cdot X_k + \\varepsilon_{| \\pmb{e} |}, \\\\\n",
    "\\mu_{| \\pmb{e}_r |} = \\alpha_{| \\pmb{e}_r |} + \\sum_{k=1}^4 \\beta_{| \\pmb{e}_r |, k} \\cdot X_k + \\varepsilon_{| \\pmb{e}_r |},\n",
    "$$\n",
    "\n",
    "The values of $X_k$ are either $1$ if the value in `\"k_id\"` is equal to $k$ or $0$. This way, the reference population is described by $\\alpha + \\varepsilon$ and the values of each $\\beta_p$ corresponds to the difference between the reference population and the measurements resulting from the $k$-th conductivity profile. If $\\beta_k \\approx 0$, the conductivity profile does not affect the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooled model\n",
    "\n",
    "The first models we fit are fully pooled. This means that we do not account for any hierarchy in the data and instead consider all the samples as independent.\n",
    "\n",
    "These models are exactly the ones described above.\n",
    "\n",
    "### Fit the models\n",
    "\n",
    "Weakly informative priors are set automatically for $\\alpha$, $\\beta$ and $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pooled_e = bmb.Model(f\"e ~ C(k_id)\", data)\n",
    "model_pooled_e_r = bmb.Model(f\"e_r ~ C(k_id)\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "trace_pooled_e = model_pooled_e.fit(draws=1000, chains=4, random_seed=RANDOM_SEED)\n",
    "trace_pooled_e_r = model_pooled_e_r.fit(draws=1000, chains=4, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "We display the probability density of $\\beta_k$ for both models as well as their $95$% HDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "idx = [\"alpha\", *[f\"beta_{k}\" for k in data[\"k\"].unique()[1:]], \"sigma\"]\n",
    "summary_pooled_e = az.summary(trace_pooled_e, stat_funcs=FUNC_DICT, extend=False)\n",
    "summary_pooled_e_r = az.summary(trace_pooled_e_r, stat_funcs=FUNC_DICT, extend=False)\n",
    "summary_pooled_e.index, summary_pooled_e_r.index = idx, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "l = len(idx[1:-1])\n",
    "fig, axs = plt.subplots(20, 2, figsize=(10, 30), sharex=\"col\")\n",
    "for i, n in enumerate(idx[1:-1]):\n",
    "    suffix = f\"{{{n.split('_')[1]}_{{{n.split('_')[-1]}}}}}\"\n",
    "    plot_posterior(\n",
    "        axs[i][0],\n",
    "        trace_pooled_e,\n",
    "        summary_pooled_e,\n",
    "        \"C(k_id)\",\n",
    "        LABELS[\"e\"] if i == l - 1 else \"\",\n",
    "        CAPTIONS[\"e\"] if i == 0 else \"\",\n",
    "        i,\n",
    "        summary_param=n,\n",
    "        show_zero=True,\n",
    "        beta_suffix=suffix,\n",
    "    )\n",
    "    plot_posterior(\n",
    "        axs[i][1],\n",
    "        trace_pooled_e_r,\n",
    "        summary_pooled_e_r,\n",
    "        \"C(k_id)\",\n",
    "        LABELS[\"e_r\"] if i == l - 1 else \"\",\n",
    "        CAPTIONS[\"e_r\"] if i == 0 else \"\",\n",
    "        i,\n",
    "        summary_param=n,\n",
    "        show_zero=True,\n",
    "        beta_suffix=suffix,\n",
    "    )\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "display_side_by_side([summary_pooled_e, summary_pooled_e_r], [\"\", \"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical model\n",
    "\n",
    "Now, we consider the fact that multiple records were obtained with the same subject.\n",
    "\n",
    "To account for this, the definitions of the intercept $\\alpha$ and the slopes $\\beta_k$ become\n",
    "$$\n",
    "\\alpha_i = \\alpha_\\text{common} + \\alpha_{\\text{sub}_i}, \\\\\n",
    "\\beta_{k, i} = \\beta_{\\text{common}, k} + \\beta_{\\text{sub}_i, k}.\n",
    "$$\n",
    "\n",
    "Here, we differenciate the common value of the intercept $\\alpha_\\text{common}$ and of the slopes $\\beta_{\\text{common}, k}$ from the subject specific contributions $\\alpha_{\\text{sub}_i}$ and $\\beta_{\\text{sub}_i, k}$. These contributions are modelled as centered normal distributions\n",
    "$$\n",
    "\\alpha_\\text{sub} \\sim \\mathcal{N}(0, \\sigma_{\\alpha_\\text{sub}}), \\\\\n",
    "\\beta_{\\text{sub}, k} \\sim \\mathcal{N}(0, \\sigma_{\\beta_\\text{sub}, k}).\n",
    "$$\n",
    "\n",
    "Hence, the models become\n",
    "$$\n",
    "| \\pmb{e} | \\sim \\mathcal{N}(\\mu_{| \\pmb{e} |, i}, \\sigma_{| \\pmb{e} |}^2), \\\\\n",
    "| \\pmb{e}_r | \\sim \\mathcal{N}(\\mu_{| \\pmb{e}_r |, i}, \\sigma_{| \\pmb{e}_r |}^2),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mu_{| \\pmb{e} |} = \\alpha_{| \\pmb{e} |, i} + \\sum_{k=1}^4 \\beta_{| \\pmb{e} |, k, i} \\cdot X_k + \\varepsilon_{| \\pmb{e} |}, \\\\\n",
    "\\mu_{| \\pmb{e}_r |} = \\alpha_{| \\pmb{e}_r |, i} + \\sum_{k=1}^4 \\beta_{| \\pmb{e}_r |, k, i} \\cdot X_k + \\varepsilon_{| \\pmb{e}_r |},\n",
    "$$\n",
    "\n",
    "### Fit the models\n",
    "\n",
    "Once again, weakly informative priors are automatically set for all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hierarchic_e = bmb.Model(f\"e ~ C(k_id) + (C(k_id) | sub)\", data)\n",
    "model_hierarchic_e_r = bmb.Model(f\"e_r ~ C(k_id) + (C(k_id) | sub)\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "trace_hierarchic_e = model_hierarchic_e.fit(draws=1000, chains=4, random_seed=RANDOM_SEED, target_accept=0.9)\n",
    "trace_hierarchic_e_r = model_hierarchic_e_r.fit(draws=1000, chains=4, random_seed=RANDOM_SEED, target_accept=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Again, we display the posteriors of $\\beta_p$ and their $95$% HDI for the two metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "idx = [\n",
    "    \"alpha\", *[f\"beta_{k}\" for k in data[\"k\"].unique()[1:]],\n",
    "    \"sigma(alpha_sub)\", *[f\"alpha_sub_{i}\" for i in range(len(data[\"sub\"].unique()))],\n",
    "    *[f\"sigma(beta_{k}_sub)\" for k in data[\"k\"].unique()[1:]],\n",
    "    *[f\"beta_{k}_{i}\" for k in data[\"k\"].unique()[1:] for i in range(len(data[\"sub\"].unique()))],\n",
    "    \"sigma\"\n",
    "]\n",
    "summary_hierarchic_e = az.summary(trace_hierarchic_e, stat_funcs=FUNC_DICT, extend=False)\n",
    "summary_hierarchic_e_r = az.summary(trace_hierarchic_e_r, stat_funcs=FUNC_DICT, extend=False)\n",
    "summary_hierarchic_e.index, summary_hierarchic_e_r.index = idx, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "l = len(idx[1:21])\n",
    "fig, axs = plt.subplots(20, 2, figsize=(10, 30), sharex=\"col\")\n",
    "for i, n in enumerate(idx[1:21]):\n",
    "    suffix = f\"{{{n.split('_')[1]}_{{{n.split('_')[-1]}}}}}\"\n",
    "    plot_posterior(\n",
    "        axs[i][0],\n",
    "        trace_hierarchic_e,\n",
    "        summary_hierarchic_e,\n",
    "        \"C(k_id)\",\n",
    "        LABELS[\"e\"] if i == l - 1 else \"\",\n",
    "        CAPTIONS[\"e\"] if i == 0 else \"\",\n",
    "        i,\n",
    "        summary_param=n,\n",
    "        show_zero=True,\n",
    "        beta_suffix=suffix\n",
    "    )\n",
    "    plot_posterior(\n",
    "        axs[i][1],\n",
    "        trace_hierarchic_e_r,\n",
    "        summary_hierarchic_e_r,\n",
    "        \"C(k_id)\",\n",
    "        LABELS[\"e_r\"] if i == l - 1 else \"\",\n",
    "        CAPTIONS[\"e_r\"] if i == 0 else \"\",\n",
    "        i,\n",
    "        summary_param=n,\n",
    "        show_zero=True,\n",
    "        beta_suffix=suffix\n",
    "    )\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "rows = [\"alpha\", \"sigma(alpha_sub)\"]\n",
    "for i in range(1, 21):\n",
    "    rows += [f\"beta_halton_{i}\", f\"sigma(beta_halton_{i}_sub)\"]\n",
    "rows += [\"sigma\"]\n",
    "display_side_by_side([summary_hierarchic_e.loc[rows, :], summary_hierarchic_e_r.loc[rows, :]], [\"\", \"\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainweb-tdcs-analysis",
   "language": "python",
   "name": "brainweb-tdcs-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
